\chapter{Conclusions}%
\label{chap:conclusion}
This thesis aims to create a robot framework that can combine three topics; learning object dynamics, the \ac{NAMO} problem and nonprehensile pushing. The overwhelming research in the individual topics stand is contrast to the sparse number of recent papers that combine the three aforementioned topics. This absence of research can be motivated by the emerged challenges when combining the three topics such as the uncertainty in planning and the complexity class of the combined problem.\bs

The main research question, that is: \textit{How do learned objects' system models improve global task planning for a robot with nonprehensile push manipulation abilities over time?} Is answered in \Cref{chap:hgraph_and_kgraph} that describes the proposed framework. Action sequences are generated by starting a search from the desired outcome and searching backward to the current configuration with the backward search technique. The chapter shows how learned system knowledge can be leveraged by alternating between searching for an action sequence and executing an action sequence.\bs

There were two aspects that improve global task planning as a result of learning object models. The first improvement is due to classification of objects, that improves the success rate of generated action sequences. Mainly because, probabilistic action sequences that may involve pushing unmovable objects cannot occur when object classification is present. During task execution the proposed framework gathers experience by storing action feedback after successful or unsuccessful actions. As a result the proposed framework converges toward an best strategy to manipulate objects which is the second improved aspect. The best strategy consists of the best combination of controller and system model that yield desirable metrics (success rate and \acl{PE}) out of the available combinations of controllers and system models. Evidence for improvements of global task planning as result of learned object dynamics is found in \Cref{chap:results} that presents the results. In this chapter the proposed robot framework is tested by providing a robot with a multiple tasks in various environments. These tests point out that task execution improves when the classification of an object is known, and when experience is gathered and leveraged in the robot environment.\bs

Research subquestion~\ref{researchsubquestion:does_it_work}, that is: \textit{How to combine learning and planning for push and drive applications?} An answer is given in \Cref{chap:required_background,chap:proposed_planning,chap:hgraph_and_kgraph}. Here it is shown how the proposed framework, that consists of the \acl{halgorithm}, the \acl{hgraph} and the \acl{kgraph} work together to combine learning and planning with the technique of backward search. The curse of dimensionality and an \ac{NP-hard} problem are bypassed by searching only in a single mode of dynamics instead of searching in composite space. The \ac{hgraph} is a graph-based structure that presents how the \ac{halgorithm} is trying to complete a subtask. The structure of the \ac{hgraph} is used to enforce the backward search technique, in which the \ac{halgorithm} searches from the target configuration toward the start configuration. Newly gained environmental knowledge is stored in the \ac{kgraph}, which firstly classifies objects as movable or unmovable, and secondly holds information on how they can be best manipulated. The \ac{kgraph} can be filled with newly learned environmental knowledge and can be queried for action suggestions.\bs

Research subquestion~\ref{researchsubquestion:does_it_remember}, that is: \textit{
To what extend is the combination of the three topics; learning object dynamics, the \ac{NAMO} problem and nonprehensile pushing influenced by environmental experience?} The answer is provided in \Cref{chap:results} that present the results. By once solving tasks in a randomized environment and remembering environmental experience, and once solving the same tasks without storing environmental experience. The results indicate that leveraging environmental experience significant improvements are made. These improvements were measured with the total task execution time and \acl{PE}, such improvements are due to selecting the best available controller and system model combination. Two important factors determine convergence toward an optimal selection of controller and system model combination. First the success factor, because it determines which experience of control and system model combination is the best choice to manipulate a specific object. Second, the set of available controllers and system models, because the proposed framework can only converge to the best available controller in the set of available controllers. During testing analytic models were used that cannot model a wide variety of systems accurately. \Cref{table:sota_vs_results_proposed} shows therefore a \xmark/\cmark, concluding that the proposed framework can partly combine the three topics. The analytic models could be replaced by adding a system identification module that is moved to the future work section, if implemented the proposed framework can claim that the three topics can fully be combined.\bs

Research subquestion~\ref{researchsubquestion:does_it_compare}, that is: \textit{How does the proposed framework compare against the state-of-the-art?} The proposed framework was able to perform better compared to the state-of-the-art. The proposed framework was tested against an existing methods that only combines a subset of the 3 main topics. Five state-of-the-art papers are selected to compare against where the success rate, planning time, execution time and number of replanning can be used to compare. Only a single comparison was made where search-, execute- and total time to complete a task is compared. From these results, it can be concluded that the proposed framework learns relatively fast compared to \citeauthor{wang_affordancebased_2020}.\bs

The proposed framework combines the three topics, the \ac{NAMO} is tackled by extending an existing path planner that detects blocked paths. The \ac{halgorithm} then combines the extended path planner with the backward search technique to search for action sequences. Action sequences consists of drive and push actions that improve over time by reviewing actions and storing action review in the \ac{kgraph}. Thereby, nonprehensile pushing, learning and the \ac{NAMO} problem are combined in the proposed framework. Since the proposed framework classifies objects and stores action feedback, it is concluded that the three topics are partly combined. The proposed framework improves upon the state-of-the-art by a significant margin in search- and execution time.\bs
