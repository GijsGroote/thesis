\chapter{Results}%
\label{chap:results}
\textit{This chapter presents various robot environments and tasks that challenge the proposed framework. The test results provide evidence that supports the answers to the research questions. The chapter starts by presenting metrics that measure task performance, the simulation environment is introduced, then a number of system models is presented and tuning variables are specified. With the system models defined and tuning parameters set test are presented, a randomized environment that is split in task that mainly involve driving and tasks that mainly involve pushing. The chapter finishes with a comparison with the state-of-the-art in \Cref{sec:compare_with_related_papers}.\bs}

\paragraph{Metrics to Measure Task Performance}
The proposed frameworks task performance is measured in metrics, these include the \ac{PE}, search-, execute- and total time to complete a task. We can draw conclusions from the evolution of the metrics over time. Furthermore, the method metrics will be used to compare the proposed framework to state-of-the-art. Now, the method metrics are presented in~\Cref{table:proposed_method_metrics}.\bs

\noindent
\begin{table}[H]
\caption{Method metrics employed to measure task performance by the proposed robot framework in the left column. Motivation on relevance of the metric is provided in the corresponding right column.}\label{table:proposed_method_metrics}
\centering
\begin{tabular}%
  {>{\raggedright\arraybackslash}p{0.25\textwidth}%
   >{\raggedright\arraybackslash}p{0.65\textwidth}}
\acl{PE} [meter] &  The \ac{PE} is high in two scenario's, first when unexpected behaviour occurs, second when the system model used to calculate the \ac{PE} describes the system poorly. When the \ac{PE} is low, that would indicate the robot encounters expected behaviour and that the model describes the system accurate. If the \ac{PE} is lowering during task execution over time, that would indicating the robot is learning to select models that are more accurate and less unexpected behavior occurs.\\
% Total Average\newline \acl{TE}& The total average \ac{TE} is created by averaging over every hypothesis' average \ac{TE} in a \ac{hgraph}. Seeing the total average \ac{TE} lower over time would indicate the robot is selecting better suitable controllers and system models, indicating the robot is learning.\\
% Final poses and\newline displacement errors & The final pose and displacement error is a metric which how a controller performs. This thesis does not create or investigate controllers, but it is interesting to see why different controllers are preferred for different objects. The final pose and displacement error could be the cause.\\
% The ratio between the number of hypotheses and the number of tasks & Expected is that whilst learning system models, the hypothesis created will be more effective. Thus the ratio between the total number of hypotheses and the total number of tasks is expected to lower with new knowledge.\\
% The ratio between the number of successful and the number of total edges in \ac{kgraph} & When the \ac{kgraph} improves recommending a controller and system model, the ratio between successful edges and total edges is expected to increase because, with better recommendations, more edges will be completed.\\
search time [sec]& The time the proposed framework spends searching for hypotheses. The search time is dominated by path planning because compared to path estimation, updating the \ac{hgraph} it is time consuming. The number of subtasks in a task, the environment and the number of unknown and classified objects influence the number of paths to be planned and the duration of planning, and thus the search time.\\ 
execution time [sec] & The time the proposed framework spends executing hypotheses. The execution time is dominated by path length, driving or pushing toward a target pose on the other side of the workspace takes more time compare to a target pose closeby the initial pose.\\ 
total time [sec] & search time + execution time.\\
\end{tabular}
\end{table}

\paragraph{The Simulation Environment}
Testing in a simulation environment has been done using the URDF Gym Environment~\cite{spahn_urdfenvironment_2022}, a 100\% python environment build upon the PyBullet library~\cite{coumans_pybullet_2016}. The code created during the thesis can be found on \href{https://gitlab.tudelft.nl/airlab-delft/msc_projects/msc_gijs_groote}{GitLab} and \href{https://github.com/GijsGroote/semantic-thinking-robot}{GitHub}. Experiments are taken on laptop with specifications: Laptop: HP ZBook Studio x360 G5, OS:~Ubuntu 22.04.1 LTS x86\_64, CPU: Intel i7-8750H (12) @ 4.100GHz, GPU: NVIDIA Quadro P1000 Mobile. The simulation environment provides many different robots, of which one robot is selected to perform tests, the point robot, which is are displayed in \Cref{fig:example_robots}. The point robot takes an velocity input along the \gls{x}- and in \gls{y}-axis.\bs

Robot input is defined as:\\
\[ u(k) = {[ u_{\gls{x}}(k), u_{\gls{y}}(k) ]}^\top \]

During testing three controllers have been used. A \ac{MPC} and \ac{MPPI} controller for driving the robot, and a \ac{MPPI} controller for robot pushing. The \ac{MPC} and \ac{MPPI} control methods were introduced in \Cref{sec:control_methods}. Three system models are implemented, a \ac{LTI} model describing robot driving, and two nonlinear model describing the robot pushing an object. First a short textual description of the available system models is provided below. Second, the state-space model is provided for the three implemented models.\bs

\begin{table}[H]
\caption{The left column displays the available models of drive- and push systems, accompanied by a description in the corresponding right column.}\label{table:available_system_models}
\centering
\begin{tabular}%
  {>{\raggedright\arraybackslash}p{0.25\textwidth}%
   >{\raggedright\arraybackslash}p{0.65\textwidth}}
\textit{lti-drive-model} & A second order \ac{LTI} model that is compatible with both the \ac{MPC} and the \ac{MPPI} drive controller. The state variables are the robot's \gls{x} and \gls{y} position. The next state $x(k+1)$ is based on the current state $x(k)$, the time step \gls{DT} and robot system input $u(k)$.\\
\textit{nonlinear-push-model-1} & A nonlinear model that is compatible with only the \ac{MPPI} push controller. The state variables are the robot's and the pushed object's \gls{x} and \gls{y} position. The next state $x(k+1)$ is based on the current state $x(k)$, the time step \gls{DT} and robot system input $u(k)$.\\
\textit{nonlinear-push-model-2} & 
A nonlinear model that is compatible with only the \ac{MPPI} push controller. The state variables are the robot's and the pushed object's \gls{x} and \gls{y} position and the objects orientation $\gls{theta}_\mathit{obj}$. The next state $x(k+1)$ is based on the current state $x(k)$, the time step \gls{DT}, robot system input $u(k)$ and geometrical variables $v_p$ and $s_t$.\\
\end{tabular}
\end{table}

State-space representation of the \textit{lti-drive-model}:\bs

\begin{equation}
\label{eq:lti-drive-model}
x_{\mathit{lti-drive-model}}(k+1)=
\begin{bmatrix}
\gls{x}_\mathit{robot}(k+1)\\
\gls{y}_\mathit{robot}(k+1)
\end{bmatrix}
=
\begin{bmatrix}
\gls{x}_{\mathit{robot}}(k) + \gls{DT} u_{\gls{x}}(k)\\
\gls{y}_{\mathit{robot}}(k) + \gls{DT} u_{\gls{y}}(k)
\end{bmatrix}
\end{equation}

State-space representation of the \textit{nonlinear-push-model-1}:\bs

\begin{equation}
x_{\mathit{nonlinear-push-model-1}}(k+1)=
\begin{bmatrix}
\gls{x}_{\mathit{robot}}(k+1)\\
\gls{y}_{\mathit{robot}}(k+1)\\
\gls{x}_{\mathit{obj}}(k+1)\\
\gls{y}_{\mathit{obj}}(k+1)
\end{bmatrix}
=
\begin{bmatrix}
\gls{x}_{\mathit{robot}}(k+1) + \gls{DT} u_{\gls{x}}(k)\\
\gls{y}_{\mathit{robot}}(k+1) + \gls{DT} u_{\gls{y}}(k)\\
\gls{x}_{\mathit{obj}}(k+1) + \frac{1}{2} \gls{DT} u_{\gls{x}}(k)\\
\gls{y}_{\mathit{obj}}(k+1) + \frac{1}{2} \gls{DT} u_{\gls{y}}(k)
\end{bmatrix}
\label{eq:nonlinear-push-model-1}
\end{equation}


State-space representation of the \textit{nonlinear-push-model-2}:\bs
\begin{equation}
x_{\mathit{nonlinear-push-model-2}}(k+1)=
\begin{bmatrix}
\gls{x}_{\mathit{robot}}(k+1)\\
\gls{y}_{\mathit{robot}}(k+1)\\
\gls{x}_{\mathit{obj}}(k+1)\\
\gls{y}_{\mathit{obj}}(k+1)\\
\gls{theta}_{\mathit{obj}}(k+1)
\end{bmatrix}
=
\begin{bmatrix}
\gls{x}_{\mathit{robot}}(k) + \gls{DT} u_{\gls{x}}(k)\\
\gls{y}_{\mathit{robot}}(k) + \gls{DT} u_{\gls{y}}(k)\\
\gls{x}_{\mathit{obj}}(k) + \gls{DT}\cos(\gls{theta}_{\mathit{obj}}(k)) (1-|\frac{2\mathit{s_t}}{H}|)\mathit{v_p}\\
\gls{y}_{\mathit{obj}}(k) + \gls{DT}\sin(\gls{theta}_{\mathit{obj}}(k)) (1-|\frac{2\mathit{s_t}}{H}|)\mathit{v_p}\\
\gls{theta}_{\mathit{obj}}(k) + \frac{2*\gls{DT}*\mathit{v_p}*\mathit{s_t}}{H}
\end{bmatrix}
\label{eq:nonlinear-push-model-2}
\end{equation}


Where distance $s_t$ and velocity $v_p$ can be visually seen in \Cref{fig:robot_and_square_object}. A positive $s_t$ indicates the object will rotate anticlockwise, a negative $s_t$ indicates a clockwise rotation. The width of an object is defined as \textit{H} and is set to 2 meters.\bs

\begin{figure}[H]
    \centering
    \includegraphics[width=7cm]{figures/results/robot_and_square_object.drawio}
    \caption{Schematic overview of point robot pushing a box object.}%
    \label{fig:robot_and_square_object}
\end{figure}

Where $v_p$ is the velocity of the robot perpendicular to the object defined as:\bs
\[ \mathit{v_p} = u_{\gls{x}} \cos(\gls{theta}_{\mathit{obj}}(k)) + u_{\gls{y}}\sin(\gls{theta}_{\mathit{obj}}(k)) \]

Two drive action edge parameterizations are available: (\ac{MPC}, \textit{lti-drive-model}) and (\ac{MPPI}, \textit{lti-drive-model}), and two push action edge parameterizations are available: (\ac{MPPI}, \textit{nonlinear-push-model-1}) and (\ac{MPPI}, \textit{nonlinear-push-model-2}).\bs

The goal of this thesis is not to find optimal control, or to model the environment with great accuracy. The goal is to select the best edge parameterization in the available set of edge parameterizations. Eleborating the control design and system model design is therefore out of the scope of this thesis, the control design parameters such as prediction horizon, cost function, number of rollouts can be found in the implementation.\bs

% \section{Benchmark Tests}%
% \label{sec:benchmark_tests}
% Three benchmark test are presented, starting with the blockade task. A large part of the proposed framework is system identification, for testing however no system identification is performed. Instead, several analytic system models are used, and the \ac{kgraph} finds which system model is the best choice for an object over time. Where the best choice is defined using the edge metrics discussed in \Cref{subsec:edge_metrics}. The analytic system models are not opting for modelling the drive or push model as accurately as they possibly can, thus severe model mismatch should be expected. Such model mismatch is no issue, to complete a subtask stable closed-loop control is required, when an edge parameterization is unable to provide closed-loop stable control, the edge will fail because a fault will be detected. To answer the research question, improvement over time should be made. Gaining such improvement does not require accurate system models, time spend improving the hand-coded system models does thus not change the result.\bs
%
%
% \paragraph{Blockade} In the blockade environment the robot is tasked with placing a box in a target pose that is blocked by a cylinder object.\bs
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.9\textwidth]{figures/results/blockade}
%     \caption{The blockade environment with the target ghost pose for the blue box. The green walls are unmovable whilst both the blue box and cylinder are movable.}%
%     \label{fig:benchmark_blockade}
% \end{figure}
%
% The blockade task neatly shows that the \ac{halgorithm} uses the backward search technique. First, the \ac{halgorithm} plans to push the box directly to the target pose, then it realizes a blocking object must first be moved to free the path. It makes the mistake of pushing the unmovable wall, then it succeeds in pushing the movable cylinder out of the way. The \ac{kgraph} then ensures that this mistake will not occur again because is remembered that the wall is immovable. Over time the \ac{kgraph} indicates that it prefers to use the \ac{MPPI} controller with the nonlinear-push-model-2 to push both the box and the cylinder object. Converging to this conclusion improves the method metrics for the task as can be seen in \Cref{fig:results_blockade}\bs
%
% \todo[inline]{is nonlinear-push-model-2 still the preferred model after testing??}
%
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.9\textwidth]{figures/tests/404_not_found}
%
%     \caption{Some results are still under development for the blockade environment}%
%     \label{fig:results_blockade}
% \end{figure}
% \todo[inline]{Test to create results for running the blockade environment, and input into above 404 not found}
%
% \paragraph{Swap}
% In the swap environment the robot should swap the locations of the 2 objects in the environment.\bs
%
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.9\textwidth]{figures/results/swap}
%     \caption{The swap environment, the robot is tasked with swapping the poses of the cylinder and the box.}%
%     \label{fig:benchmark_swap}
% \end{figure}
% The swap task shows that the \ac{halgorithm} can handle overlapping subtasks. The \ac{halgorithm} handles a single subtask at a time and randomly selects which subtask to handle next. The result for the swap task is that first, the robot will place the box on the location of the cylinder. The cylinder is blocking the path and is thus pushed to free the path, then the robot drives back to the box to push the box to its target pose. The cylinder can directly be pushed toward its target pose, because there is a free path, and the task is successfully completed. By manual inspection, this is the most efficient action sequence to complete the swap task. But there is an assumption because the initial environment has a distance between the robot and box (robot-box distance), and robot and cylinder (robot-cylinder distance) that is equal. If the initial robot-box distance is greater than the initial robot-cylinder distance, it would be more efficient to first drive toward the cylinder because that distance is smaller. The selection of subtask is random, thus there is a 50\% chance that the robot selects a subtask resulting in driving more than is necessary to complete the swap subtask.\bs
%
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.9\textwidth]{figures/results/404_not_found}
%     \caption{Some tests are still under development for the swap environment}%
%     \label{fig:results_swap}
% \end{figure}
%
% \todo[inline]{Test to create results for running the swap environment}
%
% \paragraph{Surrounded} In the surround environment the robot has to learn which box is movable to escape the enclosure of boxes.\bs
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.9\textwidth]{figures/results/surrounded}
%     \caption{The surround environment, the robot is tasked with escaping the surrounding enclosure by driving to the target ghost pose displayed on the right side in the figure. Every box objects is unmovable except the red box which is movable.}%
%     \label{fig:benchmark_surround}
% \end{figure}
% The surround task show that having and gaining environment knowledge can greatly improve task execution. Simply knowing if an object can be interacted with may lower task execution time drastically as can be seen in \Cref{fig:results_surround}.\bs
%
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.9\textwidth]{figures/results/404_not_found}
%     \caption{Some test are still under development for the surround environment}%
%     \label{fig:results_surround}
% \end{figure}
%
% \todo[inline]{Test to create results for running the surround environment}

\section{Driving and Pushing Experiments}%
\label{sec:randomization}
To improve the relevance of the experimental results, driving and pushing tasks are performed with randomized settings. A set of multiple tasks is given to the robot, named a \textit{run}. When the robot is tasked with the first task in the run the \ac{kgraph} is empty, and when the last task in the run is completed the \ac{kgraph} is filled with the experience gained. For each run, a set of random objects and properties are generated. This remains the same for the whole run, however for each task in the run the environment is reshuffled. A reshuffled does three things, first, it resets the robot pose, secondly, it gives the objects in environment new initial pose. Thirdly, new target poses are generated for the subtask in the new task. By solving multiple reshuffled tasks in a reshuffled environment, the robot gains experience that is stored in the \ac{kgraph}.\bs

\Cref{table:configure_rand_env} presents a set of parameters that initializes the random environment. Two type of tasks solved by the robot, a driving task, where the robot must drive toward multiple random target poses, and a pushing task, where the robot must push a movable object toward a random target pose.\bs

\noindent
\begin{table}[H]
\caption{The tuning parameters to initialize a random environment}%
\label{table:configure_rand_env}
\centering
\begin{tabular}%
{>{\raggedright\arraybackslash}p{0.25\textwidth}%
>{\raggedright\arraybackslash}p{0.65\textwidth}}
The \textit{size of the grid} & length and width of the ground plane in \gls{x} and \gls{y} direction.\\
The \textit{minimal and maximal size of objects} & A box will have sides with a length that lie in the specified range from minimal to maximal length. Cylinders will have a diameter and height that is within the specified range, additionally, cylinders are not higher than the radius of the cylinder to prevent cylinders from tipping over. \\
The \textit{maximal weight} & which is uniformly distributed for the environment objects, minimal weight is set by default to 1 kilogram. \\
The \textit{number of unmovable objects} & Specify the amount of unmovable objects.\\
The \textit{number of movable objects} & Specify the amount of movable objects.\\
The \textit{number of subtasks in a task} & Specify the amount of subtasks in a task.\\
The \textit{number of boxes and cylinders} & For every new object generated there is a 50\% chance it becomes a box and 50\% chance it becomes a cylinder.
\end{tabular}
\end{table}

An randomly initialized environment that is then reshuffled can be visualized in \Cref{fig:random_environment_reshuffle}.\bs

\begin{figure}[H]
    \centering
    \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/results/random1}
    \caption{Environment after initialization.}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/results/random2}
    \caption{Environment after a reshuffle.}
    \end{subfigure}

    % \vspace{0.2cm}
    % \begin{subfigure}{.49\textwidth}
    % \includegraphics[width=\textwidth]{figures/results/random3}
    % \caption{Environment after second reshuffle.}
    % \end{subfigure}
    % \hfill
    % \begin{subfigure}{.49\textwidth}
    % \centering
    % \includegraphics[width=\textwidth]{figures/results/random4}
    % \caption{Environment after thirth reshuffle.}
    % \end{subfigure}
    \caption{A random environment initialized by tuning parameters presented in \Cref{table:configure_rand_drive_env_values} and randomness.}%
    \label{fig:random_environment_reshuffle}
\end{figure}

\subsection{A Driving Task}%
\label{subsec:rand_driving}
For the driving task the random environment is created with the following tuning parameters.\bs

\begin{table}[H]
\caption{The selected tuning parameters for the randomized drive environment.}%
\label{table:configure_rand_drive_env_values}
\centering
\begin{tabular}%
{>{\raggedright\arraybackslash}p{0.30\textwidth}%
>{\raggedright\arraybackslash}p{0.60\textwidth}}
\text{grid size}  &\gls{x}=12 m, \quad \gls{y}=12 m \\
\text{object size}  &$\mathit{min\_length}=0.2 m, \quad \mathit{max\_length}=2 m$ \\
\text{object weight}  &$\mathit{max\_weight}=1000 kg$\\
\text{number of objects}  &$\mathit{num\_unmovable\_obj}=3, \quad \mathit{num\_movable\_obj}=5$ \\
\text{number of tested runs}  &$\mathit{num\_runs}=10$\\
\text{number of tasks in a run}  &$\mathit{num\_tasks}=10$\\
\text{number of subtasks in a task}  &$\mathit{num\_subtasks}=3$
\end{tabular}
\end{table}

These parameters have been specifically selected, starting with the size of the ground floor. The ground floor should be large enough such that objects can be pushed around, note that, for a driving task, pushing is involved when a path must be freed. An enormous (100 by 100 meter) ground floor would result in a longer computational time for path planning, which is undesired. A 12 by 12 meter ground floor is selected because the floor is large enough for objects to be pushed around. The range that determines the size of objects is set such that objects can be as large as the robot itself, and be around 10 times as large as the robot. With these sizes the robot is unable to grasp objects, a gripper would be too small to grasp objects. The comparatively large size fits the objective of nonprehensile pushing, there simply is no other method to manipulate such large objects other than pushing. A real-life example are can be found in harbours where tug boats push giant cargo ships around that are many times over the size of the tug boat. The ratio of solid obstacles vs.~movable objects determines if a task is more navigation (only solid obstacles) or more \ac{NAMO} (only movable objects). A task that tends toward \ac{NAMO} is favoured because that is the target environment in this thesis. There should be some unmovable obstacles that reward the robot learning such objects are unmovable (to then not interact with them). Thus there are more movable objects than solid obstacles chosen, whilst still having 2 solid obstacles around. Ten runs are taken, each run consisting of 10 tasks, the results are averaged to reach statistic relevance in a randomized environments. The number of subtasks is set to 3, a low number of drive subtasks that can be completed in under 2 minutes.\bs

Lastly, a number of tuning parameters must be set for the \ac{halgorithm}. These are the maximal robot speed, set to 1 $\frac{\mathit{meter}}{\mathit{s}}$, the \textit{cell size} for the path estimator set to 0.1 meter, the path planner takes four tuning parameters; the \textit{step size} set to 0.2 meter, the \textit{search size} set to 0.35 meter, the \textit{MovableSpaceCost} set to 2 meter and the \textit{UnknownSpaceCost} set to 3.5 meter.\bs

Given the above parameters, all parameters are now set. Ten runs are solved by the robot where every run is composed of ten tasks. The first generated environment in the run determines the type of objects and their properties that remain unchanged for the entire run. The subsequent environments in the run are reshuffled versions of the first environment. After ten runs the task performance is collected that is expressed in the metrics just described. The ten runs metrics are vertically augmented such that they are grouped by the same number of tasks in experience. First, a boxplot is presented displaying execution-, search- and total times over the ten runs whilst using \ac{kgraph} action suggestions in \Cref{fig:random_drive_time_kgraph}. The number of solved subtasks can be calculated by multiplying the number of runs times the number task in a run by the number of subtasks in a task, which is $10 \cdot 10 \cdot 3 = 300$ subtasks.\bs

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/results/random_drive_time_kgraph}
    \caption{Search-, execution- and total time to complete a drive task \textbf{whilst} using \ac{kgraph} action suggestions. The horizontal axis indicates the number of task experience in a run. The vertical axis displays a boxplot of the search-, execution- and total time over ten runs grouped by number of tasks in experience.}%
   \label{fig:random_drive_time_kgraph}
\end{figure}

The above figure displays results where the \ac{halgorithm} initially selects a random edge parameterization, after executing an action edge it sends action feedback to the \ac{kgraph}. When every possible edge parameterization for drive actions has corresponding action feedback in the \ac{kgraph}, the \ac{kgraph} starts suggesting edge parameterizations. Now a boxplot is presented in \Cref{fig:random_drive_time_no_kgraph} that displays task execution without using the \ac{kgraph} action suggestions, instead a random parameterization is selected for every action edge. By fixing a seed the environments used for \Cref{fig:random_drive_time_kgraph} are equal to the environments used for \Cref{fig:random_drive_time_no_kgraph}.\bs

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/results/random_drive_time_no_kgraph}
    \caption{Search-, execution- and total time to complete a drive task by randomly selecting edge parameterizations. The horizontal axis indicates the number of task experience in a run. The vertical axis displays a boxplot of the search-, execution- and total time over ten runs grouped by number of tasks in experience. }%
   \label{fig:random_drive_time_no_kgraph}
\end{figure}

Lets compare the means from \Cref{fig:random_drive_time_kgraph,fig:random_drive_time_no_kgraph} and compare task execution with and without the \ac{kgraph} action suggestions. However overall \Cref{fig:random_drive_time_kgraph} shows significant improvement over \Cref{fig:random_drive_time_no_kgraph} which becomes better visible if only the means are compared in \Cref{fig:random_drive_time_vs}.\bs

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/results/random_drive_time_vs}
    \caption{Comparing average total time to complete a task out of then runs for a driving task. For the edge parameterizations once with the use of \ac{kgraph} action suggestions are leveraged indicated by \quotes{with \ac{kgraph}} and once with random selection indicated by \quotes{no \ac{kgraph}}.}\label{fig:random_drive_time_vs}
\end{figure}

Two driving edge parameterizations are available, the \ac{MPC} and the \ac{MPPI} parameterization that both use lti-drive-model. The \ac{kgraph} prefers the \ac{MPC} parameterization over the \ac{MPPI} parameterization as can be seen in \Cref{table:rand_drive_mpc_vs_mppi}. In this table is can be seen that the \ac{kgraph} needs at most one task of experience to suggest the \ac{MPC} parameterization. An explanation why there was no trend to be found in \Cref{fig:random_drive_time_kgraph}, it was already converged at the first task.  

\begin{table}[H]
    \caption{The selection of the (\ac{MPC}, \textit{lti-drive-model}) parameterization versus selecting the (\ac{MPPI}, \textit{lti-drive-model}) parameterization for drive actions during the randomized driving tasks. The leftmost column indicates that tasks execution is performed with the \ac{kgraph} action suggestions, and without action suggestions.}%
    \label{table:rand_drive_mpc_vs_mppi}
    \centering
    \begin{tabular}%
      {%
        >{\raggedright\arraybackslash}p{0.10\textwidth}
        >{\raggedright\arraybackslash}p{0.22\textwidth}
      |p{0.4cm}p{0.4cm}p{0.4cm}p{0.4cm}p{0.4cm}p{0.4cm}p{0.4cm}p{0.4cm}p{0.4cm}p{0.4cm}}
      \multicolumn{2}{c|}{Number of Tasks in experience} &0&1&2&3&4&5&6&7&8&9\\\toprule
      \multirow{4}{0.1\textwidth}{With \ac{kgraph} suggestions} 
      &Number of \ac{MPC} parameterizations&22&33&34&33&33&33&33&34&33&32\\
      &Number of \ac{MPPI} parameterizations&11&0&0&0&0&0&0&0&0&0\\
      & \ac{MPC} selected in total drive actions [\%]&67&100&100&100&100&100&100&100&100&100\\
      & \ac{MPPI} selected in total drive actions [\%]&33&0&0&0&0&0&0&0&0&0\\\midrule
      \multirow{4}{0.1\textwidth}{Without \ac{kgraph} suggestions} 
      &Number of \ac{MPC} parameterizations &14&14&12&12&17&16&17&19&16&11\\
      &Number of \ac{MPPI} parameterizations &19&19&21&21&17&17&18&14&17&22\\
      & \ac{MPC} selected from total drive actions [\%] &42&42&36&36&50&48&49&58&48&33\\
      & \ac{MPPI} selected from total drive actions [\%]&58&58&64&64&50&52&51&42&52&67\\
    \end{tabular}
\end{table}

The \ac{halgorithm} successfully completes 300 driving subtasks (3 subtasks per task, 10 tasks per run, 10 runs are completed). Once while storing edge feedback in the \ac{kgraph} that suggest edge parameterization, and once without \ac{kgraph}. In \Cref{table:rand_drive_mpc_vs_mppi} the number of \ac{MPC} and \ac{MPPI} parameterizations should be more than 30 for any number of tasks in experience. In many cases it is more than 30, which indicates more than 30 drive edges were created to complete 30 drive subtasks. A fault has been detected that terminates the execution of an edge, to complete the task, a new edge is created, resulting in more than 30 \ac{MPC} and \ac{MPPI} edge parameterizations. When comparing both a positive effect is measured of the use of the \ac{kgraph} on the total task required to complete a task compared to random selection of edge parameterization. Now a task is taken that compares the use of the \ac{halgorithm} with and without \ac{kgraph} for a pushing task.\bs
\todo{put this here: First the search- and execution time of a task are investigated, and its development over multiple tasks. Then a comparison is made between solving and reshuffling multiple tasks once with the \ac{kgraph} that suggests edge parameterizations, and once without suggestions, by randomly selecting a available edge parameterization.}

\subsection{A Pushing Task}%
\label{subsec:rand_pushing}
The push task in the randomized environment consists of a single subtask. To complete this push task the robot must push the object toward its specified target pose. The tuning parameters that make up the random environment for the push task can be visualized in \Cref{table:configure_rand_push_env_values}.\bs
\todo{This test of yours, 10 is the magic number does Winters support that claim?G}
\todo{\cite{dewinter_using_} can this <- winter guy add something to my conclusions, especially because it does have outliers wich are far out. }

\begin{table}[H]
\caption{The selected tuning parameters for the randomized push environment.}%
\label{table:configure_rand_push_env_values}
\centering
\begin{tabular}%
{>{\raggedright\arraybackslash}p{0.30\textwidth}%
>{\raggedright\arraybackslash}p{0.60\textwidth}}
\text{grid size}  &\gls{x}=12 m, \quad \gls{y}=12 m \\
\text{object size}  &$\mathit{min\_length}=0.2 m, \quad \mathit{max\_length}=2 m$ \\
\text{object weight}  &$\mathit{max\_weight}=1000 g = 1 \mathit{kg}$\\
\text{number of objects}  &$\mathit{num\_unmovable\_obj}=3, \quad \mathit{num\_movable\_obj}=5$ \\
\text{number of tested runs}  &$\mathit{num\_runs}=10$\\
\text{number of tasks in a run}  &$\mathit{num\_tasks}=6$\\
\text{number of subtasks in a task}  &$\mathit{num\_subtasks}=1$
\end{tabular}
\end{table}




\begin{figure}[H]
    \centering
    \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/results/random_1}
    \caption{}
    \end{subfigure}

    \vspace{0.2cm}
    \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/results/random_2}
    \caption{}
    \end{subfigure}
    \caption{Two random environments where the pushing task contains a single subtask displayed by a target ghost pose.}%
    \label{fig:random_environnment}
\end{figure}

All tuning parameters are set up, now the results are presented, first the pushing task is completed using \ac{kgraph} suggestions. The search-, execute- and total time for task completion is presented in \Cref{fig:random_push_time_kgraph}. Then the same tasks are completed without help of the \ac{kgraph} suggestions in \Cref{fig:random_drive_time_no_kgraph}. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/results/random_push_time_kgraph}
    \caption{Search-, execution- and total time to complete a pushing task \textbf{with} \ac{kgraph} action suggestions. The horizontal axis indicates the number of task experience in a run. A run contains ten tasks, starting the run with an empty \ac{kgraph} that collects action feedback as the robot gains experience. The vertical axis displays a boxplot of the search-, execution- and total time over ten runs, where the sum of search- and execution time equals total time.}%
    \label{fig:random_push_time_kgraph}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/results/random_push_time_no_kgraph}
    \caption{Search-, execution- and total time to complete a pushing task \textbf{without} \ac{kgraph} action suggestions. The horizontal axis indicates the number of task experience in a run. A run contains ten tasks, starting the run with an empty \ac{kgraph} that collects action feedback as the robot gains experience. The vertical axis displays a boxplot of the search-, execution- and total time over ten runs, where the sum of search- and execution time equals total time.}%
    \label{fig:random_push_time_no_kgraph}
\end{figure}

Both \Cref{fig:random_push_time_kgraph} and \Cref{fig:random_push_time_no_kgraph} look very similar due to solving the same tasks. Whilst gaining more experience the \ac{halgorithm} that uses the \ac{kgraph} suggestions yields a better total task completion time. Which is easier to see if only the mean of the total tasks times are plotted in \Cref{fig:random_push_time_vs}. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/results/random_push_time_vs}
    \caption{Comparing average total time to complete a task out of then runs for a pushing task. For the edge parameterizations once with the use of \ac{kgraph} action suggestions are leveraged indicated by \quotes{with \ac{kgraph}} and once with random selection indicated by \quotes{no \ac{kgraph}}.}\label{fig:random_push_time_vs}
\end{figure}


\begin{table}[H]
    \caption{The selection of the (\ac{MPPI}, \textit{nonlinear-push-model-1}) parameterization versus selecting the (\ac{MPPI}, \textit{nonlinear-push-model-2}) parameterization for push actions during the randomized pushing tasks. The leftmost column indicates that tasks execution is performed with the \ac{kgraph} action suggestions, and without action suggestions.}
    \label{table:rand_push_model1_vs_model2}
    \centering
    \begin{tabular}%
      {
        >{\raggedright\arraybackslash}p{0.10\textwidth}
        >{\raggedright\arraybackslash}p{0.35\textwidth}
      |p{0.4cm}p{0.4cm}p{0.4cm}p{0.4cm}p{0.4cm}p{0.4cm}}
      \multicolumn{2}{c|}{Number of Tasks in experience} &0&1&2&3&4&5\\\toprule
      \multirow{4}{0.1\textwidth}{With \ac{kgraph} suggestions} 
      &Number of \textit{nonlinear-push-model-1} parameterizations&4&5&8&10&10&10\\
      &Number of \textit{nonlinear-push-model-2} parameterizations&5&4&2&0&0&0\\
      & \textit{nonlinear-push-model-1} selected in total push actions [\%]&44&56&80&100&100&100\\
      & \textit{nonlinear-push-model-2} selected in total push actions [\%]&56&44&20&0&0&0\\\midrule
      \multirow{4}{0.1\textwidth}{Without \ac{kgraph} suggestions} 
      &Number of \textit{nonlinear-push-model-1} parameterizations&4&3&7&5&4&5\\
      &Number of \textit{nonlinear-push-model-2} parameterizations&5&5&3&5&6&4\\
      & \textit{nonlinear-push-model-1} selected in total push actions [\%]&44&38&70&50&40&56\\
      & \textit{nonlinear-push-model-2} selected in total push actions [\%]&56&62&30&50&60&44\\
    \end{tabular}
\end{table}


The \ac{kgraph} favours the \ac{MPPI} controller with \textit{nonlinear-push-model-1} as can be seen in \Cref{table:rand_push_model1_vs_model2}. The nonlinear-push-model-1 parameterization does not only have a lower execution time compared to the nonlinear-push-model-2 parameterization, it also has a lower \ac{PE} as can be seen in \Cref{fig:random_push_pe_vs}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/results/random_push_pe_vs}
    \caption{Box plot of the \acl{PE} for solving reshuffled tasks. The horizontal axis indicates the number of task experience in a run. A run contains six tasks, starting the run with an empty \ac{kgraph} that collects action feedback as the robot gains experience. The vertical axis displays a boxplot of \acl{PE} over six runs. For the edge parameterizations once with the use of \ac{kgraph} action suggestions are leveraged indicated by \quotes{with \ac{kgraph}} and once with random selection indicated by \quotes{no \ac{kgraph}}.}%
    \label{fig:random_push_pe_vs}
\end{figure}

The proposed framework has been compared against itself, it has shown that learning improves the method metrics by a notable margin as can be seen in \Cref{fig:random_push_pe_vs}. Now the proposed framework is compared to the state-of-the-art in the upcoming section.\bs

\section{Comparison with State-of-the-Art}%
\label{sec:compare_with_related_papers}
In the introduction \Cref{table:sota_and_3_topics} was presented. That table presents state-of-the-art methods and the subset of the three main topics that they include (the three topics; learning system models, the \ac{NAMO} problem and nonprehesile pushing). Now that table presented again in \Cref{table:sota_vs_results_proposed}, an additional column is augmented to the table on the right side. This extra column indicates the testing metric that state-of-the-arts uses and underlines the metric that is compared between the proposed framework and the state-of-the-art.\bs

\noindent
\begin{table}[H]
\caption{Overview of recent state-of-the-art papers that include a subset of the 3 topics (learning system models, \ac{NAMO}, and nonprehensile pushing). The method metric indicates the testing method used by the paper, where the underlined metric is used to compare against the proposed framework.}%
\label{table:sota_vs_results_proposed}
  \centering
  \rowcolors{2}{white}{myEvenLighterColor}
  \begin{tabular}
    {>{\raggedright\arraybackslash}P{1.5cm}%
      >{\raggedright\arraybackslash}P{1.0cm}%
      >{\centering\arraybackslash}P{1.4cm}%
      |>{\centering\arraybackslash}P{1.4cm}%
      >{\centering\arraybackslash}P{1.4cm}%
      |>{\centering\arraybackslash}P{1.4cm}%
      >{\centering\arraybackslash}P{1.4cm}%
      |>{\raggedright\arraybackslash}P{3.0cm}
    }
    &&& \multicolumn{2}{c|}{\ac{NAMO}} & \multicolumn{2}{c}{\shortstack[c]{Specify object\\target poses}}\\
  Author&
  Citation&
  Learns\newline object\newline dynamics&
  \vspace{-0.2cm}\rotatebox{50}{prehesile}&
  \vspace{-0.4cm}\rotatebox{50}{nonprehesile}&
  \vspace{-0.2cm}\rotatebox{50}{prehesile}&
  \vspace{-0.4cm}\rotatebox{50}{nonprehesile}&
  method metric\\\toprule
  \citeauthor{ellis_navigation_2022} &          \cite{ellis_navigation_2022} &          \cmark& \xmark& \cmark& \xmark& \xmark& success rate\\
  \citeauthor{sabbaghnovin_model_2021} &        \cite{sabbaghnovin_model_2021} &        \cmark& \cmark& \xmark& \cmark& \xmark& success rate, execution time prediction error, final pose error\\
  \citeauthor{scholz_navigation_2016} &         \cite{scholz_navigation_2016} &         \cmark& \cmark& \xmark& \xmark& \xmark& runtime, planning time, number of replanning number of calls to update model\\
  \citeauthor{vega-brown_asymptotically_2020} & \cite{vega-brown_asymptotically_2020} & \xmark& \cmark& \xmark& \cmark& \xmark& computation time\\
  \citeauthor{wang_affordancebased_2020} &      \cite{wang_affordancebased_2020} & \cmark& \xmark& \cmark& \xmark& \xmark& \underline{computation and} \underline{execution time}\\
  Groote & Proposed Framework & \xmark/\cmark& \xmark& \cmark& \xmark& \cmark&
\end{tabular}
\end{table}

A comparison with one state-of-the-art paper is made, that is accomplished by recreating the environment that the state-of-the-art has used during testing. With \citeauthor{wang_affordancebased_2020} the computation- (or search) and execution time is compared.\bs

% \paragraph{Comparing Success Rate with \citeauthor{ellis_navigation_2022}}
% \todo{This can use some extra review here, it is unclear}
% \citeauthor{ellis_navigation_2022} claims to push with a success rate \todo{Corrado: perhaps eleborate on that 100 percent, how does ellis define that?} of a 100\%~\cite{ellis_navigation_2022}. The test environment that \citeauthor{ellis_navigation_2022} has used is very similar to the random pushing task discussed in \Cref{sec:randomisation}. For both the random driving task and the random pushing task the success rate is 100\%. Meaning that every driving or pushing subtask was successfully completed. It is even so that for the result of the random push task (displayed in \Cref{fig:rand_push_full_pred,fig:random_push_all_times,fig:random_push_with_without_kgraph}) the number of hypotheses is equal to the number of subtasks \todo{Corrado: How do I see this? unclear to tthe reader}. Meaning that every subtask is completed by the first hypothesis generated. Both similar experiments from \citeauthor{ellis_navigation_2022} and this thesis have a 100\% success rate for similar
% , allowing us to conclude that the results are comparable.



% \paragraph{Comparing something TODO with \citeauthor{sabbaghnovin_model_2021}}
% \citeauthor{sabbaghnovin_model_2021} grasp-pushes and grasp-pulls a walker object to two new target configurations~\cite{sabbaghnovin_model_2021}. The task is mimicked by pushing a box object of equal dimensions to the same target configurations. Compared to \citeauthor{sabbaghnovin_model_2021} average of 125 and 160 seconds to complete task 1 and task 2 the proposed framework takes an average of only 27 and 33 seconds respectively (10 seconds computation time) to complete similar tasks. The decrease in total time allows to conclude that the proposed frameworks improve upon the method proposed by~\citeauthor{sabbaghnovin_model_2021}.\bs \todo{Corrado: So your pushing controllers are performin better than prehensile manipulation? amazing, but doub}



% \paragraph{Comparing something TODO with \citeauthor{vega-brown_asymptotically_2020}}
% \citeauthor{vega-brown_asymptotically_2020} has a task that pushes two boxes into a goal region. It takes around 300 seconds to return a hypothesis~\cite{vega-brown_asymptotically_2020}. \todo{Corrado: What do you mean? for the next sentce}Both pushing tasks can be compared with two separate pushing tasks from the random push environment. By doubling the time to push a single object to its target configuration, $2 \cdot 30 = 60$ seconds both tasks could be compared. However, the point of \citeauthor{vega-brown_asymptotically_2020} paper is that a global minimal task is sought. The proposed framework randomly selects a subtask and tries to complete it to then move to the next randomly selected subtask. A global minimum is thus not sought and both papers can therefore not be compared properly.\todo{Corrado: Then why including it in the comparison?}\bs



% \paragraph{Comparing Computation and Execution Time }
% Three state-of-the-art papers that use computation and execution time as the testing metrics are compared to the proposed framework. To accomplish a fair comparison the environments that have been used in the \todo{Corrado: Where do I see this?} citations are rebuilt in the pybullet software. The results are then directly compared to conclude that the proposed framework is as good as or even better in terms of computation time and execution time for similar tasks.\bs


\paragraph{Comparing Computation and Execution time with \citeauthor{wang_affordancebased_2020}}
\citeauthor{wang_affordancebased_2020} combines the \ac{NAMO} problem with learning object dynamics. He tests his method with a task to drive toward a target pose, where a chair is blocking the path~\cite{wang_affordancebased_2020}. \citeauthor{wang_affordancebased_2020} proposed framework takes an accordance approach compared with a contact-implicit motion planning algorithm~\cite{wang_affordancebased_2020}. A test is performed in a real-world environment, where the robot drive toward a target pose, whilst the path is blocked by a chair. The real-world robot environment is implicitly presented as simulation environment. The environment is mimicked with three walls and a red box on the spot where is chair stands. The implicit representation of the real-world environment and the mimicked robot environments can be seen in \Cref{fig:wang}.\bs

\begin{figure}[H]
    \centering
    \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/results/wang_env}
    \caption{\citeauthor{wang_affordancebased_2020} simulation environment.}%
    \label{subfig:wang_env}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/results/wang_mimick}
    \caption{Recreated robot environment.}%
    \label{subfig:wang_mimick}
    \end{subfigure}
    \caption{Two similar environments and tasks, the robots are tasked to drive toward a target pose indicated with the green ghost poses. In both environment a direct path is blocked by the red box.}%
    \label{fig:wang}
\end{figure}

\citeauthor{wang_affordancebased_2020} solves the task three times, the average search-, execute- and total time over these three executions is displayed in \Cref{table:wang_vs_mimick}. Note that, during testing \citeauthor{wang_affordancebased_2020} splits the search time in two categories; time to estimate accordance of an object and time to optimize the trajectory. The recreated or mimicked environment is displayed in \Cref{subfig:wang_mimick} and is solved ten times, every time starting without environmental knowledge and thus an empty \ac{kgraph}. The average search, execute- and total time over these then executions is also displayed in \Cref{table:wang_vs_mimick}.\bs

\begin{table}[H]
    \caption{Average execute-, search and total times for a task that involves learning object dynamics and the \ac{NAMO} problem. The results are average over three solved tasks for \citeauthor{wang_affordancebased_2020}, and ten solved tasks for the proposed framework. A visualization of the task if presented in \Cref{fig:wang}.}%
    \label{table:wang_vs_mimick}
    \centering
    \begin{tabular}%
    {>{\raggedright\arraybackslash}p{0.2\textwidth}|%
    >{\centering\arraybackslash}p{2cm}%
    >{\centering\arraybackslash}p{2cm}}%
    Author &\citeauthor{wang_affordancebased_2020} & Groote \\\toprule
    search time [sec]  & 109 & 26 \\
    execution time [sec]  & 67 & 4 \\
    total time [sec] & 176 & 30
    \end{tabular}
\end{table}

The proposed framework outperforms the contact-implicit motion planning framework proposed by \citeauthor{wang_affordancebased_2020}. Mainly the search time improves by a margin. The execution time improves, but because \citeauthor{wang_affordancebased_2020} tests in an real-world environment where motion equations are more complex compared to the simulated environment, an improvement in the execution time is expected.\bs


% \paragraph{Comparing Prediction Error}
% \citeauthor{sabbaghnovin_model_2021} displays the final position errors for a selection of objects which range from 0.05 to 0.6 meters. A success threshold is set to 0.1 meters, which determines that the object is at its target location~\cite{sabbaghnovin_model_2021}. The success threshold in this thesis is set to a staggering 0.9 meters,\todo{Corrado: But then how can you compare success rate if you have different metrics for success?} thus it is concluded an object has reached its target position, whilst it is still 0.9 meters from its target configurations. This thesis does not try to reach optimal control, it tries to select the best controller in the available set of controllers. If the same controller from \citeauthor{sabbaghnovin_model_2021} would be available, then similar final position errors would be obtained. It can be concluded that the prediction error \todo{Corrado: Not position errro?}of the proposed framework is worse compared to this state-of-the-art. The reason is that the proposed framework focuses on improving the control selection and action sequences over time, and not on lowering final prediction errors.\bs

% \paragraph{Comparing Number of Replanning times}
% \todo[inline]{\cite{scholz_navigation_2016}}
% \todo{recreate this envionrment, run 10 tests and compare the search and execution time, as well at the number of call toward the planner}
% \todo{additinoally execute this environment a twice in a row, show that the execution number of call to the planner drops, theand the execution time drops}




\todo{Would it be possibel to create extra results that support your work?, or some benchmark tests?}
