\subsection{Edge Metrics}%
\label{subsec:edge_metrics}
The \ac{kgraph} keeps an orderd list of `good' and `bad' edge arguements (controller and system model). `Good' and `bad' are defined by edge metrics, these metrics are created after completion of an edge, regardless of the edge successfully completed or failed. An indication is given on why certain metrics matter in \cref{table:review_edge_metrics}.

\begin{table}[htb!]
\centering
\begin{tabular}[t]{p{3.7cm} p{10cm}}
  \acf{PE}&  To better compare prediction errors the \ac{PE} is summarised and average \ac{PE}. The average \ac{PE} is a indicator of a accurate system model, but can give a misleading results, since \ac{PE} also in an indicator of unexpected collisions. Prediction error should thus only be used if there are not collisions detected. The average \ac{PE} comes with more flaws, since the average is mostly determined by outliers, some unfortunate outliers in the \ac{PE} might for the largest part determine the average \ac{PE}. The average \ac{PE} whill thus not be used because it is not robust enough.\\
\acf{TE}& For a low \ac{TE} the system model must be a close to the real motion equations to yield a feasible path, the controller must be well tuned to be able to track that path and the controller and system model must be in collaboration, because the controller uses the system model to calculate system input. A low \ac{TE} tells multiple things, whilst a high \ac{TE} would indicate improvements could be gained in the controller, the system model or their collaboration.\\
ratio \#succesfully completed edges and \#total edges & Over time the \ac{kgraph} can recommend the same edge arguements multiple times. Loggin the ratio of succeding edges vs total edges builds an evident protofilio. Still this metric has to be taken with a grain of salt, because edges with equal edge arguements perform similar actions e.g.~pushing an object through a wide corridor is compared to pushing the same object through a narrow corridor. One could say: \quotes{comparing apples with pears}.\\
final position and \newline displacement error & The quality of the end result is measured in the final position and displacement error. The importants should thus be stressed when ordering edge arguements.\\
planning time& With system identifiaction, path estimation, motion or manipulation planning the planning time can vary in orders of magnitude between simple or more complex approaches. Planning time mainly serves to rank the slowest planners low, whilst not influencing the rank of fast and average planners.\\
runtime& Also known as execution time, it would be a quality indicator if start and target states would be equal. Edges are recommended to solve similar tasks, where path legnth between start and target state is different. Thus planning time is not of any use to rank edges.\\
completion time = \newline runtime + planning time & With a same arguementation as runtime, completion time is not of any use to rank edges.\\
\end{tabular}
\caption{Edge metrics used to rank control methods from `good' to `bad'}
\label{table:review_edge_metrics}
\end{table}
